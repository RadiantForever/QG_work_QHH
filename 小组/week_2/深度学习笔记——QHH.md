# 深度学习笔记

## 深度学习定义：是机器学习的一个子领域，它使用受大脑神经网络结构和功能启发的算法。

在深度学习中使用的神经网络并不是真正的生物神经网络，是与生物神经网络共享一些特征，因此，我们称它们为*人工*神经网络 （ANN）

![image-20250329100949161](https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250329100949322.png)

1. 人工神经网络是使用所谓的神经元构建的。
2. ANN 中的神经元被组织成我们所说的层。
3. ANN 图层（除输入和输出图层之外的所有图层）称为隐藏图层。
4. 如果 ANN 具有多个隐藏层，则称该 ANN 为深度 ANN。

## 人工神经网络（artificial neural network）定义：由一组称为神经元的连接单元组成，这些单元被组织成我们所说的层（输入、输出、隐藏层）。

![image-20250329101657458](https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250329101657588.png)

### 位于输入和输出图层之间的图层称为隐藏图层。

![image-20250329100949161](https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250329100949322.png)

###### 在这张图中，

###### 蓝色的第一层，输入层，由八个节点组成，每个节点都代表数据集中给定样本中的单个特征。

而第一层到第二层的输入计算中，引入权重的概念。

## 权重(weights)

当信息作为输入传入到神经元的时候，神经元会分配给每一个信息一个***相关权重***，然后将输入的信息乘于相应的权重，就是该信息的输入。在开始的时候，神经元会初始化每个信息的权重，之后根据相应的反馈和模型训练来更新每个信息的权重。

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250329105343627.png" alt="image-20250329105343561" style="zoom:33%;" />

## 激活函数（activation function）

### 将输入信号转化为输出信号的函数，节点输出 = 输入的加权和

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250329105600519.png" alt="image-20250329105600453" style="zoom:33%;" />

这个是常见的sigmoid函数表达式与它的图像，阈值是0.5

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250329105740956.png" alt="image-20250329105740918" style="zoom:50%;" />

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250329105803764.png" alt="image-20250329105803622" style="zoom:50%;" />

## 训练模型过程

###### 人工智能的训练其实就是在大量的运算中寻找误差更小时候的参数组合，可以抽象不停计算，不停寻找最佳的参数组合，以此获得对某个函数的最佳拟合，就像泰勒展开一样。

## 偏差(bias)

输出预测结果的期望与样本真实结果的差距，就是真实值与预测值的偏差。通过这个东西，我们弄了一堆误差计算算法，交叉熵，RMSE,MAE,MAPE，R平方等等。

## 常用的算法：一般有监督学习与半监督学习

其中监督学习：通过一系列标准输入输出数据，产生最佳的模拟函数，到达输入input-----函数给出最佳output

分为两大类：一类是回归(regression)(比如高中常学的拟合函数),一类是分类(classification)，分类是无限输入-----有限输出，比如西瓜书中的好瓜还是坏瓜，肿瘤大小-------肿瘤良性or恶性。



<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250203173908444.png" alt="image-20250203173908268" style="zoom: 25%;" />



<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250204164005523.png" alt="image-20250204164005370" style="zoom:25%;" />

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250204165551413.png" alt="image-20250204165551249" style="zoom:25%;" />

半监督学习：在未贴上标签的数据中发现一些特定的特征or结构

比如:clustering(聚类)，将没有标签的数据自动归类放置到不同集群中，如新闻集群：含关键词“熊猫”，“双胞胎”，“动物园”。



## 无监督学习（unsupervised learning）

输入数据没有被标记，也没有确定的结果。样本数据类别未知，需要根据样本间的相似性对样本集进行分类（聚类，clustering）试图使类内差距最小化，类间差距最大化。通俗点将就是实际应用中，不少情况下无法预先知道样本的标签，也就是说没有训练样本对应的类别，因而只能从原先没有样本标签的样本集开始学习分类器设计。

###### 非监督学习目标不是告诉计算机怎么做，而是让计算机自己去学习怎样做事情。非监督学习有两种思路。第一种思路不为其指定明确分类，而是在成功时，采用某种形式的激励制度。需要注意的是，这类训练通常会置于决策问题的框架里，因为它的目标不是为了产生一个分类系统，而是做出最大回报的决定，可以对正确的行为做出激励，而对错误行为做出惩罚。

无监督学习的方法分为两大类：

(1)  一类为基于概率密度函数估计的直接方法：指设法找到各类别在特征空间的分布参数，再进行分类。

(2)  另一类是称为基于样本间相似性度量的简洁聚类方法：其原理是设法定出不同类别的核心或初始内核，然后依据样本与核心之间的相似性度量将样本聚集成不同的类别。

利用聚类结果，可以提取数据集中隐藏信息，对未来数据进行分类和预测。应用于数据挖掘，模式识别，图像处理等。

## 两者的不同点

1. 有监督学习方法必须要有训练集与测试样本。在训练集中找规律，而对测试样本使用这种规律。而非监督学习没有训练集，只有一组数据，在该组数据集内寻找规律。
2. 有监督学习的方法就是识别事物，识别的结果表现在给待识别数据加上了标签。因此训练样本集必须由带标签的样本组成。而非监督学习方法只有要分析的数据集的本身，预先没有什么标签。如果发现数据集呈现某种聚集性，则可按自然的聚集性分类，但不予以某种预先分类标签对上号为目的。
3. 非监督学习方法在寻找数据集中的规律性，这种规律性并不一定要达到划分数据集的目的，也就是说不一定要“分类”。

## 何时采用哪种方法

 简单的方法就是从定义入手，有训练样本则考虑采用监督学习方法；无训练样本，则一定不能用监督学习方法。但是，现实问题中，即使没有训练样本，我们也能够凭借自己的双眼，从待分类的数据中，人工标注一些样本，并把它们作为训练样本，这样的话，可以把条件改善，用监督学习方法来做。对于不同的场景，正负样本的分布如果会存在偏移（可能大的偏移，可能比较小），这样的话，监督学习的效果可能就不如用非监督学习了。

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250205092311010.png" alt="image-20250205092310802" style="zoom: 25%;" />

## 无监督学习有聚类学习，降维算法，异常检测等等。



## 寻找多元导数最小值的实质

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250206200451998.png" alt="41bb7d30d90bcd744e7047cb4d98ab4" style="zoom:33%;" />

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250206200508200.png" alt="0338560611f46b8666f4eefe825df6a" style="zoom:33%;" />

关键点：寻找到处在位置下降最快的方向。

梯度下降算法又通常称为批量梯度下降算法。批量梯度下降每次学习都使用整个训练集，因此这些计算是冗余的，因为每次都使用完全相同的样本集。但其优点在于每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点(凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点)，但是其缺点在于每次学习时间过长，如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新。

## training set(训练集)

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207164338566.png" alt="image-20250207164338396" style="zoom:33%;" />

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207165008989.png" alt="image-20250207165008609" style="zoom:25%;" />

![image-20250207165828998](https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207165829177.png)

## 代价曲线的计算演示，通过j（w）——w图寻找代价最小的参数

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207170346454.png" alt="image-20250207170346124" style="zoom: 25%;" />



<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207171105730.png" alt="image-20250207171105549" style="zoom: 33%;" />

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207172350705.png" alt="image-20250207172350268" style="zoom: 25%;" />

3D降维成2D的一种绘图，如图的三个不同颜色的线，他们在代价曲线的登高线上，意味着不同的参数组合也会导致相同的代价值，而最中间位置的点便是最小误差的参数组合

## 过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，模型在训练集上表现很好，但在测试集上却表现很差。模型对训练集"死记硬背"（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，泛化能力差。

解决方法：

1.增加训练数据数

 发生过拟合最常见的现象就是数据量太少而模型太复杂，过拟合是由于模型学习到了数据的一些噪声特征导致，增加训练数据的量能够减少噪声的影响，让模型更多地学习数据的一般特征

2.使用正则化约束 

在代价函数后面添加正则化项，可以避免训练出来的参数过大从而使模型过拟合。使用正则化缓解过拟合的手段广泛应用，不论是在线性回归还是在神经网络的梯度下降计算过程中，都应用到了正则化的方法减少特征数

3.欠拟合需要增加特征数，那么过拟合自然就要减少特征数。去除那些非共性特征，可以提高模型的泛化能力